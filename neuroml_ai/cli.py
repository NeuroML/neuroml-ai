#!/usr/bin/env python3
"""
Main runner interface for nml-ai

File: neuroml_ai/cli.py

Copyright 2025 Ankur Sinha
Author: Ankur Sinha <sanjay DOT ankur AT gmail DOT com>
"""

import asyncio
import logging
import subprocess
from contextlib import chdir
from pathlib import Path

import typer

from neuroml_ai.rag.rag import NML_RAG
from neuroml_ai.mcp.server import codegen
from mcp import ClientSession, StdioServerParameters
from mcp.client.streamable_http import StreamableHTTPTransport

nml_ai_app = typer.Typer()


@nml_ai_app.command()
def nml_ai_cli(
    chat_model: str = "ollama:qwen3:1.7b",
    embedding_model: str = "ollama:bge-m3",
    gui: bool = False,
    single_query: str = "",
    regen_vector_stores: bool = False,
):
    """NeuroML AI cli wrapper function"""
    print("*** NeuroML AI chat assistant ***")
    print("Please note that answers are generated by LLMs and may be incorrect.")
    print()
    print("Type 'quit' to exit.")
    print()
    print()

    if not gui:
        async def cli_main():
            """Cli main async"""
            from yaspin import yaspin

            # TODO: continue here: define a client, but check whether we want
            # to stick to the python sdk or move to fastmcp
            mpc_client = 
            await mcp_client.initialize()
            tools = await mcp_client.list_tools()
            print(f"Available tools: {[tool.name for tool in tools.tools]}")

            nml_ai = NML_RAG(
                mcp_client,
                chat_model=chat_model,
                embedding_model=embedding_model,
                logging_level=logging.DEBUG,
            )
            await nml_ai.setup()

            if regen_vector_stores:
                nml_ai.stores.remove()
                nml_ai.stores.load()

            if len(single_query):
                print(f"NeuroML-AI (USER) >>> {single_query}\n\n")

                if single_query == "quit":
                    pass
                else:
                    with yaspin(text="Working ..."):
                        response = await nml_ai.run_graph_invoke(single_query)
                        print(f"NeuroML-AI (AI) >>> {response}\n\n")

            else:
                while (query := input("NeuroML-AI (USER) >>> ")) != "quit":
                    assert nml_ai

                    # we use checkpoints, so we don't need to store and reload the
                    # state ourselves
                    with yaspin(text="Working ..."):
                        response = await nml_ai.run_graph_invoke(query)
                    print(f"NeuroML-AI (AI) >>> {response}\n\n")

        try:
            print("Running!")
            asyncio.run(cli_main())
        except KeyboardInterrupt:
            print("\nInterrupted. Exiting.")

    else:
        # streamlit app
        cwd = Path(__file__).parent
        with chdir(cwd):
            subprocess.run("streamlit run streamlit_ui.py".split())

    print("NeuroML-AI >>> Bye!")


if __name__ == "__main__":
    nml_ai_app()

#!/usr/bin/env python3
"""
Main runner interface for nml-ai

File: neuroml_ai/cli.py

Copyright 2025 Ankur Sinha
Author: Ankur Sinha <sanjay DOT ankur AT gmail DOT com>
"""

import typer
import logging
import subprocess
from pathlib import Path
from contextlib import chdir
from neuroml_ai.rag import NML_RAG
from neuroml_ai.schemas import AgentState, QueryTypeSchema, EvaluateAnswerSchema


nml_ai_app = typer.Typer()


@nml_ai_app.command()
def nml_ai_cli(
    chat_model: str = "ollama:qwen3:1.7b",
    embedding_model: str = "ollama:bge-m3",
    gui: bool = False,
):
    """NeuroML AI cli wrapper function"""

    if not gui:
        from yaspin import yaspin

        print("*** NeuroML AI chat assistant ***")
        print("Please note that answers are generated by LLMs and may be incorrect.")
        print()
        print("Type 'quit' to exit.")
        print()
        print()

        nml_ai = NML_RAG(
            chat_model=chat_model,
            embedding_model=embedding_model,
            logging_level=logging.DEBUG,
        )
        nml_ai.setup()

        # persistent state
        state = {}

        while (query := input("NeuroML-AI (USER) >>> ")) != "quit":
            assert nml_ai
            # refresh state
            state.update({"query": query, "query_type": QueryTypeSchema(),
                          "text_response_eval": EvaluateAnswerSchema(),
                          "message_for_user": ""})

            with yaspin(text="Working ..."):
                state = nml_ai.run_graph_invoke_state(AgentState(state))

            if message := state.get("message_for_user", None):
                print(f"NeuroML-AI (AI) >>> {message}\n\n")
            else:
                print("I was unable to answer")
    else:
        # streamlit app
        cwd = Path(__file__).parent
        with chdir(cwd):
            subprocess.run("streamlit run streamlit_ui.py".split())

    print("NeuroML-AI >>> Bye!")


if __name__ == "__main__":
    nml_ai_app()
